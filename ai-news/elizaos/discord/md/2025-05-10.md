# elizaOS Discord - 2025-05-10

## Overall Discussion Highlights

### ElizaOS v2 Development Status
- **Beta Status**: ElizaOS v2 is currently in beta and in developers' hands
- **Release Timeline**: No hard deadline set; will be shipped "when it's ready"
- **New Features**: Agents can now have custom tabs for plugins
- **MCP Integration**: Three Multi-Chain Protocol plugins are currently on the development radar

### Technical Implementation Challenges
- **Local AI Integration**: Multiple users reported configuration issues with Ollama and LM Studio in the v2 beta
- **Plugin Architecture**: Ollama support has been moved to a dedicated plugin requiring specific environment variables
- **Text Embedding Limitations**: Current implementation may require OpenAI despite attempts to use alternatives like Ollama or Anthropic

### Governance Evolution
- Moving away from traditional DAO structures toward a new approach with ElizaOS at its core
- Developing "soft governance" concepts (non on-chain voting) as an alternative model
- Potential to create a governance blueprint for other projects

## Key Questions & Answers

### ElizaOS v2 Features & Timeline
- **Q**: When is v2 coming?  
  **A**: "Thoon" - The beta is in developers' hands with no hard deadline, will ship when ready

- **Q**: Any MCP vision for ElizaOS?  
  **A**: "There's 3 MCP plugins on my radar at the moment"

- **Q**: What is ai16z soft governance?  
  **A**: Non on-chain voting and beginning of engagement around governance, rethinking traditional DAO approaches

### Technical Configuration
- **Q**: How do I configure local Ollama models in ElizaOS beta?  
  **A**: You need to use the dedicated Ollama plugin with specific environment variables like OLLAMA_API_ENDPOINT, OLLAMA_SMALL_MODEL, etc.

- **Q**: What's causing the "StudioLM text generation error: fetch failed" with LM Studio?  
  **A**: Make sure the models are available in LM Studio

- **Q**: How can I set up ElizaOS v2 to not respond to everything on Twitter/X and only reply when tagged?  
  **A**: Changing your character name might help as it can be confusing to LLM, and adding action examples in your configuration

## Community Help & Collaboration

### Local AI Model Integration
- **bob_the_spounge** helped **Void** with configuring Ollama with ElizaOS beta by providing the correct environment variable format and explaining that Ollama was moved to its own plugin
- **bob_the_spounge** assisted **Sthx** with Twitter/X integration issues, suggesting configuration changes to avoid spam detection and bans

### Development Support
- **Patt** and **Kenk** directed **Void** to appropriate resources for launching a local Eliza agent, pointing to the #dev-support channel for technical assistance
- **bob_the_spounge** shared troubleshooting steps for LM Studio errors, suggesting to check if models are properly available

## Action Items

### Technical
- Install Ollama plugin separately with "elizaos create" then add plugin to character configuration (mentioned by bob_the_spounge)
- Configure Ollama environment variables correctly (OLLAMA_API_ENDPOINT, OLLAMA_SMALL_MODEL, etc.) (mentioned by bob_the_spounge)
- Ensure models are available in LM Studio before attempting to use them (mentioned by bob_the_spounge)
- Implement custom tabs for plugins in agents (mentioned by shaw)
- Develop and integrate MCP plugins (3 mentioned) (mentioned by Kenk)

### Documentation
- Document the proper way to install and configure the Ollama plugin (mentioned by bob_the_spounge)
- Provide guidance for launching local Eliza agents (mentioned by Void)

### Feature
- Develop ElizaOS v2 with improved capabilities (mentioned by multiple users)
- Create new governance model as alternative to traditional DAOs with ElizaOS at core (mentioned by Kenk)
- Support text embedding with providers other than OpenAI (mentioned by bob_the_spounge)
- Creation of a crypto-western cyberpunk story experience in Hyperfy with locations like Degen Bar, Rugpull Hill, and Rugpull County Jail (mentioned by Dr. Neuro)