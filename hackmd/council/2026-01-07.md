# Council Briefing: 2026-01-07

## Monthly Goal

December 2025: Execution excellence‚Äîcomplete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- Core reliability advanced through shipping transport-unification and critical database hardening, while field reports exposed recurring provider/embedding friction and token narrative confusion that risk developer trust if left undocumented.

## Key Points for Deliberation

### 1. Topic: Reliability Milestone: Unified Transports & Messaging Integrity

**Summary of Topic:** Engineering delivered a high-leverage stability win by unifying client hooks across HTTP/SSE/WebSocket and fixing double/triple message processing, aligning directly with execution excellence and cloud-readiness. Council should decide whether to freeze feature intake briefly to validate this foundation under real workloads before accelerating new surface-area.

#### Deliberation Items (Questions):

**Question 1:** Do we declare a short "stability gate" (hardening sprint) after transport unification to protect developer trust before pushing more features?

  **Context:**
  - `GitHub PR elizaos/eliza#6300: "feat: unified hooks with multi-transport support (HTTP/SSE/WebSocket)" and "Fixes critical double/triple message processing"`
  - `North Star principle: "Execution Excellence - Reliability and seamless UX over feature quantity"`

  **Multiple Choice Answers:**
    a) Yes‚Äî1 week stability gate with load tests, regression suite expansion, and a release candidate.
        *Implication:* Reduces incident risk and builds confidence for Cloud launch, at the cost of short-term feature velocity.
    b) Partial gate‚Äîcontinue features, but require reliability checks for anything touching messaging/transports.
        *Implication:* Balances momentum with guardrails, but increases coordination overhead and may still leak regressions.
    c) No‚Äîship forward; treat field bugs as the validation mechanism.
        *Implication:* Maximizes speed but risks visible instability that undermines developer-first trust narratives.
    d) Other / More discussion needed / None of the above.

**Question 2:** Which transport should be the "golden path" documented for new builders to minimize confusion and support load?

  **Context:**
  - `GitHub PR elizaos/eliza#6300: transport types "http" (sync), "sse" (streaming), "websocket" (async via Socket.IO) with legacy mode mapping`
  - `Discord 2026-01-05: repeated onboarding-style troubleshooting around provider ordering/keys indicates docs sensitivity to defaults`

  **Multiple Choice Answers:**
    a) SSE as default for most apps (streaming UX, simple infra).
        *Implication:* Optimizes perceived responsiveness and reduces websocket complexity, but may limit real-time bidirectional needs.
    b) HTTP as default (simplest mental model), with SSE/WebSocket as advanced upgrades.
        *Implication:* Lowest barrier for developers and tutorials, but may underwhelm compared to competitor streaming experiences.
    c) WebSocket as default for "agent OS" feel, with HTTP/SSE optional.
        *Implication:* Best for interactive multi-agent UIs, but increases operational complexity and failure modes for newcomers.
    d) Other / More discussion needed / None of the above.

**Question 3:** Do we formalize a backwards-compatibility policy for legacy 'mode' parameters and transport naming to protect ecosystem plugins and downstream forks?

  **Context:**
  - `GitHub PR elizaos/eliza#6300: "Legacy mode parameter still supported for backward compatibility (deprecated)"`
  - `Core principle: "Open & Composable"`

  **Multiple Choice Answers:**
    a) Adopt a clear deprecation window (e.g., 2 minor releases) with automated warnings and a migration guide.
        *Implication:* Preserves composability while steadily reducing tech debt, improving ecosystem reliability.
    b) Support legacy indefinitely but freeze legacy features and tests.
        *Implication:* Avoids breaking changes but accumulates maintenance burden and slows architectural evolution.
    c) Fast deprecate‚Äîremove legacy in the next minor release to simplify the stack.
        *Implication:* Accelerates internal clarity but risks fragmenting the community and breaking third-party integrations.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Provider Stack Friction: Embeddings, MCP Integration, and Model Ops Hygiene

**Summary of Topic:** Discord logs reveal a recurring integration trap: Anthropic-only setups still require an embedding pathway (OpenAI key/value + plugin ordering) and produce 'TEXT_EMBEDDING' handler errors. This is a direct DX and reliability concern; we should choose between code-level fallbacks, opinionated defaults, and documentation-first mitigations.

#### Deliberation Items (Questions):

**Question 1:** Should we implement a first-class fallback for TEXT_EMBEDDING (or explicit 'no-embedding' mode) to prevent Anthropic-only deployments from failing unexpectedly?

  **Context:**
  - `Discord 2026-01-05 (Andrei Mitrea): "No handler found for delegate type: TEXT_EMBEDDING"`
  - `Discord 2026-01-05 (sayonara / Stan ‚ö°): workaround requires OpenAI key and plugin ordering`

  **Multiple Choice Answers:**
    a) Implement runtime fallback: if no embedding provider, disable embedding-dependent features and warn loudly.
        *Implication:* Reduces hard failures and improves onboarding, but may degrade capabilities silently if warnings are missed.
    b) Ship an official lightweight embedding provider (local/cheap) and auto-configure when absent.
        *Implication:* Improves reliability and cost control, but adds maintenance surface area and model-selection complexity.
    c) Do not add fallback; enforce explicit configuration validation with a clear error and docs link.
        *Implication:* Keeps system explicit and composable, but preserves friction that can drive developers away.
    d) Other / More discussion needed / None of the above.

**Question 2:** How do we standardize secrets and key-rotation practice (Anthropic/OpenAI/etc.) without slowing contributors or leaking operational risk?

  **Context:**
  - `Discord 2026-01-05: "Team is rotating Anthropic API keys and distributing new ones to team members"`
  - `Discord 2026-01-05: "The OpenAI key doesn't need to be valid, it just needs to have some value." (Andrei Mitrea)`

  **Multiple Choice Answers:**
    a) Adopt a centralized secrets manager + short-lived tokens + documented rotation cadence for core contributors.
        *Implication:* Improves security and auditability, but requires tooling and process change that may slow fast iteration.
    b) Keep current ad-hoc rotation but add a strict configuration validator to eliminate "dummy key" behaviors.
        *Implication:* Improves correctness quickly, but leaves security posture inconsistent across teams and environments.
    c) Accept dummy keys as a compatibility shim and prioritize documentation over enforcement.
        *Implication:* Minimizes immediate friction but normalizes insecure patterns and increases future support burden.
    d) Other / More discussion needed / None of the above.

**Question 3:** What is the Council's priority response to reports of Claude code review failures‚Äîtreat as a CI reliability incident or a tooling nice-to-have?

  **Context:**
  - `Discord 2026-01-05 (Stan ‚ö°): "Investigate why Claude code review is failing and dig into CI logs"`
  - `GitHub completed item elizaos/eliza#6328: "fix(ci): allow cursor bot to trigger Claude workflows"`

  **Multiple Choice Answers:**
    a) Incident-level‚Äîassign an owner, add monitoring, and require green AI review workflows for merges touching core.
        *Implication:* Raises code quality consistency and trust, but can bottleneck shipping if the AI tooling is flaky.
    b) Medium priority‚Äîfix opportunistically and keep AI review advisory (non-blocking).
        *Implication:* Maintains velocity while improving tooling gradually, but may miss issues that AI review would catch.
    c) Low priority‚Äîpause AI review workflows until Cloud stabilizes.
        *Implication:* Reduces CI complexity now, but risks regressions and reduces perceived engineering rigor.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Token & Ecosystem Signal: Narrative Clarity, Utility Proposals, and Scam Surface

**Summary of Topic:** Community discussions show both opportunity (token-pairing monetization, AI arena/betting concepts) and risk (confusion between unrelated 'Eliza' tokens, scams, and speculative noise). Council must choose a coherent near-term token narrative aligned with shipping reliability and Cloud adoption, not merely price discourse.

#### Deliberation Items (Questions):

**Question 1:** Do we formally publish a "Token Identity & Migration" canonical explainer to preempt confusion and reduce support load‚Äîeven if migration is not the central engineering focus this week?

  **Context:**
  - `Discord 2026-01-05: user confusion "I have Eliza token and now I see ElizaOS..."; reply (Kenk): "It's not related to this project"`
  - `Monthly directive: "complete token migration with high success rate"`

  **Multiple Choice Answers:**
    a) Yes‚Äîpublish an official FAQ + pinned Discord post + docs page; keep it purely factual and linkable.
        *Implication:* Reduces reputational risk and repeated support churn, strengthening trust through clear documentation.
    b) Partial‚Äîadd Discord bot auto-replies and a short pinned message; defer full docs until next release.
        *Implication:* Quickly reduces noise, but may be insufficient if confusion spreads across X/Telegram.
    c) No‚Äîavoid token discourse; direct users to moderators and keep engineering channels focused.
        *Implication:* Protects focus but increases the chance that misinformation defines the narrative externally.
    d) Other / More discussion needed / None of the above.

**Question 2:** Which ecosystem monetization direction best supports the North Star (developer-first, reliable infra) without incentivizing low-signal speculation?

  **Context:**
  - `Discord 2026-01-05 (nancy): "Monetize the ElizaOS tech stack through token pairing"`
  - `Discord 2026-01-05: "AI arena/colosseum where AI agents compete with betting" (LikequickscopinginMW2)`

  **Multiple Choice Answers:**
    a) Cloud-first utility: token-gated discounts/credits, priority deploy slots, and verified agent badges tied to reliability SLAs.
        *Implication:* Aligns token value with real usage and developer outcomes, reinforcing trust through shipping.
    b) Ecosystem pairing/incentives: partner tokens and shared fees to drive distribution and integrations.
        *Implication:* May accelerate ecosystem reach, but risks narrative drift and complex alignment with multiple communities.
    c) Entertainment-first primitives: prediction/arena mechanics as flagship growth engine.
        *Implication:* Can create rapid attention, but may overshadow reliability goals and attract adversarial/scam dynamics.
    d) Other / More discussion needed / None of the above.

**Question 3:** How aggressively should we harden community security posture (anti-scam protocols) given rising attention and token-related confusion?

  **Context:**
  - `Discord 2026-01-05: scam incident; Hexx üåê warns and reports/bans scammer after disappearing link`
  - `Core principle: "Trust Through Shipping" (trust also includes safety and comms hygiene)`

  **Multiple Choice Answers:**
    a) High‚Äîdeploy automated link scanning, stricter DM policies, and a rapid-response moderation runbook with public reporting.
        *Implication:* Protects community trust and reduces harm, but may introduce moderation overhead and false positives.
    b) Moderate‚Äîpin safety guidance, expand moderator coverage during peak hours, and add a reporting shortcut bot command.
        *Implication:* Improves safety with manageable effort, but may not scale if attention spikes further.
    c) Minimal‚Äîhandle scams manually as they arise to avoid friction.
        *Implication:* Lowest overhead now, but risks reputational damage and user loss if incidents become visible.
    d) Other / More discussion needed / None of the above.