# Council Briefing: 2025-01-15

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- The project showed strong shipping velocity (notably onchain deployment and core memory improvements), but rising reliability gaps (Docker/cloud, duplicate responses, RAG edge cases) risk eroding developer trust unless we pivot to stabilization discipline.

## Key Points for Deliberation

### 1. Topic: Reliability vs Velocity in Core & Cloud Deployments

**Summary of Topic:** Feature throughput remains high (new agent deployment modes, memory primitives), while operational issues cluster around cloud/Docker, action duplication, and parallel performance—directly challenging the 'Execution Excellence' principle.

#### Deliberation Items (Questions):

**Question 1:** Do we declare a short-term stabilization freeze (bugfix-first) for core runtime + Docker/cloud paths, even if it slows new plugins/features?

  **Context:**
  - `GitHub issues summary: "Docker deployment problems: Issue #2343 identifies bugs when running the application in cloud environments from a Docker image."`
  - `GitHub issues summary: "Duplicate responses: Issue #2316 highlights a bug where actions receive duplicate responses."`

  **Multiple Choice Answers:**
    a) Yes—impose a timeboxed stabilization freeze on core runtime + deployment paths.
        *Implication:* Increases developer trust and reduces support load, but delays ecosystem expansion and partner deliverables.
    b) No—continue parallel feature shipping, but stand up a dedicated reliability strike team.
        *Implication:* Maintains momentum while containing risk, but requires disciplined ownership and may still leak regressions.
    c) Hybrid—freeze only high-risk surfaces (Docker/cloud, Direct Client + Postgres) while continuing low-risk features.
        *Implication:* Targets the highest-impact breakages while preserving visible shipping velocity, but adds coordination overhead.
    d) Other / More discussion needed / None of the above.

**Question 2:** Which reliability KPI should become the Council’s primary gating metric for releases over the next two cycles?

  **Context:**
  - `Daily update (Jan 15, 2025): "Reported a bug related to running in the cloud from a Docker image."`
  - `GitHub issues summary: "Performance concerns: Issue #2311 raises questions about low performance under parallel request conditions."`

  **Multiple Choice Answers:**
    a) Deployment success rate (Docker image + one-click paths) across a defined reference matrix.
        *Implication:* Optimizes first-run success and reduces onboarding friction, aligning most directly with developer trust.
    b) Runtime correctness metrics (no duplicate actions, deterministic action processing, error budget).
        *Implication:* Protects agent integrity and prevents embarrassing public bot failures, but requires better observability instrumentation.
    c) Performance under load (parallel requests throughput, memory retrieval latency, adapter consistency).
        *Implication:* Prepares for cloud-scale adoption, but risks neglecting beginner-facing setup reliability.
    d) Other / More discussion needed / None of the above.

**Question 3:** Should the Council prioritize hardening the Direct Client API (start/stop agents, Postgres stability) as the canonical control plane for ElizaOS Cloud?

  **Context:**
  - `GitHub updates summary (Jan 14): "Implemented Delete Agent functionality in Direct Client API (PR #2267)."`
  - `Recent issues: "Server crashes: Issue #2306 reports that using the Direct Client POST endpoint with Postgres causes crashes with exit status 7."`

  **Multiple Choice Answers:**
    a) Yes—make Direct Client API the control-plane priority and stabilize Postgres first.
        *Implication:* Accelerates Cloud readiness and external integrations, but concentrates risk on one interface.
    b) Not yet—keep Direct Client as experimental until core runtime invariants are fully stable.
        *Implication:* Reduces incident risk but slows platformization and managed deployment narratives.
    c) Split-path—stabilize a minimal ‘safe subset’ of Direct Client endpoints and deprecate unsafe operations temporarily.
        *Implication:* Enables Cloud progress while limiting blast radius, but may frustrate power users seeking full lifecycle control.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Composable Expansion: Onchain Agent Transformer & Cross-Chain Surface Area

**Summary of Topic:** Onchain deployment capabilities and expanding chain plugins strengthen the 'Open & Composable' thesis, but multiply the test and security burden; the Council must decide where composability ends and platform guarantees begin.

#### Deliberation Items (Questions):

**Question 1:** How do we position the Onchain Agent Transformer: flagship strategic pillar now, or experimental frontier until formal security + support guarantees exist?

  **Context:**
  - `Daily update (Jan 15, 2025): "Introduced the Onchain Agent Transformer, enabling Eliza agents to be deployed as Solidity smart contracts across 10+ blockchains (#2319)."`
  - `GitHub updates summary: "Implemented Onchain Agent Transformer to transform Eliza agents into Solidity smart contracts (PR #2319)."`

  **Multiple Choice Answers:**
    a) Flagship now—market it as a core differentiator and accelerate docs/examples.
        *Implication:* Captures mindshare quickly, but increases risk if early users treat experimental behavior as production-grade.
    b) Experimental—ship behind explicit warnings and focus on hardening before promotion.
        *Implication:* Protects trust and reduces reputational risk, but slows ecosystem excitement around ‘unstoppable agents’.
    c) Graduated rollout—select 1-2 reference chains + audited example agents, then expand.
        *Implication:* Balances hype and reliability, creating a measurable maturity path for cross-chain agent deployment.
    d) Other / More discussion needed / None of the above.

**Question 2:** Should we enforce a stricter plugin acceptance standard (tests, security review, support tiers) before expanding chain coverage further?

  **Context:**
  - `Daily update (Jan 15, 2025): "A request for tests for the Solana plugin (#2344)."`
  - `GitHub PR summary (Jan 14): "PR #2275 adds a plugin for the Tron blockchain" and "PR #2278 introduces a plugin to support the BNB chain."`

  **Multiple Choice Answers:**
    a) Yes—require minimum test coverage + security checklist for any chain plugin entering ‘supported’ status.
        *Implication:* Improves reliability and reduces exploit risk, but may slow community contributions and breadth.
    b) No—keep the barrier low; rely on community iteration and mark plugins as ‘experimental’ by default.
        *Implication:* Maximizes ecosystem growth, but increases fragmentation and support burden, potentially harming DX.
    c) Two-lane system—fast lane for experimental plugins, slow lane for supported plugins with gating criteria.
        *Implication:* Creates clarity without throttling innovation, but demands governance and labeling discipline.
    d) Other / More discussion needed / None of the above.

**Question 3:** What is the Council’s preferred value narrative for cross-chain capability: breadth (20+ chains) or depth (few chains with “production-grade” workflows and tooling)?

  **Context:**
  - `Daily report (Jan 14, tweets): "Eliza framework being available on '20+ blockchains'" (DankVR).`
  - `Partners channel framing: Shaw emphasizes production-ready agents vs full autonomy, and direct integrations as a differentiator.`

  **Multiple Choice Answers:**
    a) Breadth-first—maximize chain count and integrations to become the default agent framework everywhere.
        *Implication:* Wins ecosystem mindshare but risks uneven quality and ‘sprawling’ maintenance costs.
    b) Depth-first—select a small reference set and deliver polished end-to-end workflows + tooling.
        *Implication:* Strengthens trust and repeatable deployments, but may concede narrative territory to broader competitors.
    c) Segmented—breadth via community plugins, depth via Council-maintained ‘gold standard’ stacks.
        *Implication:* Aligns open-source scale with execution excellence, but requires explicit governance of what is ‘gold.’
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Rebrand + Tokenomics: Governance Narrative and Partner Flywheel Design

**Summary of Topic:** Trademark-driven rebranding to ElizaOS and the proposed 10% tribute/revenue-share flywheel are central to ecosystem alignment, but ambiguity on ticker change feasibility, partner SOPs, and value-accrual mechanics is generating coordination drag and community anxiety.

#### Deliberation Items (Questions):

**Question 1:** What is the Council’s priority in the rebrand execution sequence: legal/identity containment, developer-facing continuity, or partner/market messaging?

  **Context:**
  - `Partners channel: "Rebranding from ai16z to ElizaOS ... due to trademark concerns with a16z" (Shaw relayed in partners summary).`
  - `Associates channel: "voting in 1-2 months so we can just change it, we're not gonna migrate" (shaw).`

  **Multiple Choice Answers:**
    a) Legal/identity containment first—lock names, domains, and brand assets; then roll out messaging.
        *Implication:* Reduces legal and partnership risk quickly, but may leave developers temporarily confused across repos/docs.
    b) Developer continuity first—ensure packages, docs, and onboarding flows remain stable through the rename.
        *Implication:* Protects DX and trust, but delays external narrative control and partner unblocking.
    c) Partner/market messaging first—publish the narrative and token plan to prevent vacuum/FUD.
        *Implication:* Controls perception early, but risks mismatch if implementation details (ticker, migration) shift later.
    d) Other / More discussion needed / None of the above.

**Question 2:** Do we codify the ‘10% tribute’ mechanism into the stack (technical enforcement), or keep it voluntary as a social/brand compact?

  **Context:**
  - `Tokenomics channel: "DorianD suggested embedding this 10% tribute mechanism in the codebase to ensure revenue even from forked implementations."`
  - `Partners channel: Shaw’s model: projects donate tokens; DAO accrues value via revenue share and buy pressure.`

  **Multiple Choice Answers:**
    a) Technical enforcement—embed tribute hooks in critical plugins/launch pathways to protect the flywheel from forks.
        *Implication:* Strengthens value capture but may trigger backlash from open-source purists and reduce adoption.
    b) Voluntary alignment—keep tribute opt-in, focus on making the ‘Marketplace of Trust’ so valuable that teams choose it.
        *Implication:* Preserves open-source ethos and adoption, but weakens guaranteed value accrual.
    c) Mixed model—voluntary for core framework, enforced only for premium Cloud/marketplace services.
        *Implication:* Aligns incentives with paid value, but requires clear product boundaries and enforcement mechanisms.
    d) Other / More discussion needed / None of the above.

**Question 3:** What partner services should the DAO standardize first to convert ‘attention/distribution’ into a credible repeatable revenue engine?

  **Context:**
  - `Tokenomics channel: "AI16z's primary value to partners is distribution and attention" (st4rgard3n).`
  - `Tokenomics channel: proposal to explore market maker services for partner token launches (yikesawjeez).`

  **Multiple Choice Answers:**
    a) Launch + liquidity services (MM partnerships, LP tooling, listing coordination) as the first standardized offering.
        *Implication:* Creates a high-leverage partner value proposition but introduces operational and reputational risk if execution slips.
    b) Developer enablement services (plugin support, deployment templates, audits, reference implementations).
        *Implication:* Reinforces North Star (developer-first) and improves ecosystem quality, but may monetize more slowly.
    c) Information + governance services (weekly automated updates, partner SOPs, verification/disclaimer systems).
        *Implication:* Reduces coordination cost and trust gaps quickly, and supports scaling, but is less directly revenue-generative.
    d) Other / More discussion needed / None of the above.