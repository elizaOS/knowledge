# Council Briefing: 2026-01-06

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- Council attention is pulled toward execution excellence under load: critical reliability fixes and release hygiene (build memory blowups, plugin publishing/versioning gaps, and database isolation bugs) are becoming the gating factors for Cloud readiness and developer trust.

## Key Points for Deliberation

### 1. Topic: Reliability & Release Hygiene (Builds, Plugins, Data Isolation)

**Summary of Topic:** Multiple signals indicate developer trust risks from operational instability: Turbo builds reportedly consuming extreme memory, Discord plugin publishing/versioning irregularities, and a critical SQL data isolation bug requiring a robust fix with tests.

#### Deliberation Items (Questions):

**Question 1:** Do we declare a short-term reliability freeze (stability sprint) that blocks new features until build memory, plugin publishing/versioning, and data isolation paths meet a defined SLO?

  **Context:**
  - `Discord 2026-01-04: "Turbo build tool experiencing high memory usage issues (21GB+)."`
  - `Discord 2026-01-03: "Issues were reported with Discord plugin versioning... failed publishing of v1.3.4 and the jump from v1.3.3 to v1.3.5."`

  **Multiple Choice Answers:**
    a) Yes—impose a 1–2 week reliability freeze with explicit exit criteria (memory ceiling, publish pipeline checks, regression tests).
        *Implication:* Maximizes trust-through-shipping and Cloud readiness, but delays roadmap features and community-visible expansion.
    b) Partial—freeze only the affected surfaces (build tooling + plugin release pipeline), allow core UX/Cloud work to continue.
        *Implication:* Balances momentum with stability, but risks continued perception that “quality is optional” if issues persist elsewhere.
    c) No—keep feature velocity; address reliability opportunistically as issues arise.
        *Implication:* Maintains headline progress but increases probability of compounding outages and developer churn.
    d) Other / More discussion needed / None of the above.

**Question 2:** What should be the canonical mechanism to prevent plugin versioning/publishing gaps (e.g., v1.3.4 failure) from reaching users?

  **Context:**
  - `Discord 2026-01-03 (YogaFlame): "Investigate Discord plugin versioning issues (v1.3.3 to v1.3.5) | Fix failed publishing of v1.3.4."`

  **Multiple Choice Answers:**
    a) Centralized release automation (CI-managed publish) with semver enforcement and immutable release notes per plugin.
        *Implication:* Creates repeatable shipping discipline aligned with execution excellence, but requires upfront CI/platform investment.
    b) Registry-side policy: only allow versions that pass automated install+smoke tests before being indexed as “stable.”
        *Implication:* Shifts quality gates closer to users’ entry point, improving reliability without fully controlling plugin repos.
    c) Light-touch process: document best practices and rely on maintainers to follow them.
        *Implication:* Low overhead, but historically insufficient to prevent gaps and breaks in a fast-moving ecosystem.
    d) Other / More discussion needed / None of the above.

**Question 3:** How aggressively do we standardize and require the new logging format/linter across core, Cloud containers, and first-party plugins?

  **Context:**
  - `Discord 2026-01-03 (Stan ⚡): "implemented significant improvements to the logging system through PR #6263... new linter... warns developers when logs are in the wrong format."`
  - `GitHub Monthly Summary: PR #6263 "optimize provider handling in MultiStep" merged 2026-01-03.`

  **Multiple Choice Answers:**
    a) Mandatory immediately for all first-party repos (core + Cloud + plugins); block merges on violations.
        *Implication:* Fastest path to observability and supportability, but may slow contributor throughput short-term.
    b) Mandatory in core/Cloud now; phased rollout for plugins with migration tooling and warnings first.
        *Implication:* Protects the most critical surfaces while avoiding ecosystem shock from sudden breaking enforcement.
    c) Optional guidance only; prioritize feature development and rely on best-effort adoption.
        *Implication:* Minimizes friction now but increases long-term debugging cost and undermines “reliable framework” positioning.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Public Agent Ecosystem Roadmap vs. Execution Excellence

**Summary of Topic:** The GitHub signal is strong on product definition (public agent discovery, forking, and guest gating), but the Council must ensure this roadmap does not outrun reliability, Cloud launch readiness, and the “developer-first” trust contract.

#### Deliberation Items (Questions):

**Question 1:** Should the Council prioritize public agent discovery/forking as the next major growth lever, or defer until Cloud + stability metrics are green?

  **Context:**
  - `GitHub Monthly Summary: roadmap issues opened for "public agent discovery platform" (#6302) and "fork and customize existing agents" (#6305).`

  **Multiple Choice Answers:**
    a) Prioritize now—public agents become the flagship funnel into Cloud and the ecosystem.
        *Implication:* Accelerates adoption and showcases composability, but increases surface area during a stability-sensitive window.
    b) Sequence it—ship Cloud/stability gates first, then release public agent ecosystem features in a controlled wave.
        *Implication:* Aligns with execution excellence and reduces churn risk, but slows perceived product expansion.
    c) Deprioritize—focus on framework DX and plugin maturity; let public ecosystem emerge later from community.
        *Implication:* Strengthens foundations, but may miss a market window where “agent marketplaces” define mindshare.
    d) Other / More discussion needed / None of the above.

**Question 2:** What is the Council’s decision on anonymous (unauthenticated) public-agent chat limits and gating UX to optimize conversion without degrading first impressions?

  **Context:**
  - `GitHub issue #6312: "Limit messages for non-signed up user to ~2-3" (borisudovicic).`
  - `GitHub issue #6313: "Separate public agent states" describing unauthenticated vs authenticated vs owner intent (borisudovicic).`

  **Multiple Choice Answers:**
    a) Hard gate at 2–3 messages with a clean overlay prompting signup; keep UI minimal for visitors.
        *Implication:* Maximizes conversion clarity and cost control, but risks feeling restrictive if value isn’t demonstrated quickly.
    b) Soft gate: allow more messages but degrade features (no memory, no tools) until signup.
        *Implication:* Improves “wow” factor while managing abuse, but adds complexity and potential confusion in UX states.
    c) No gate: allow substantial free use and rely on later upsells (credits/features) for conversion.
        *Implication:* Best first impression, but can create runaway costs and attract low-intent traffic that harms service quality.
    d) Other / More discussion needed / None of the above.

**Question 3:** How do we answer the recurring community skepticism that ElizaOS is “just a wrapper,” using flagship experiences and documentation as proof of differentiated value?

  **Context:**
  - `Discord 2026-01-03: "Some skepticism was expressed about ElizaOS's capabilities beyond being a wrapper for AI models."`
  - `Discord 2026-01-03 Action Item: "Create clearer examples of ElizaOS capabilities... demonstrate value (Mentioned by i3)."`

  **Multiple Choice Answers:**
    a) Ship 2–3 end-to-end reference agents (Cloud-deployed) that demonstrate persistence, tool-use, and cross-platform interoperability with metrics.
        *Implication:* Most persuasive evidence aligned with trust-through-shipping, but requires coordinated engineering + DX + docs.
    b) Focus on docs first: a “Why ElizaOS” page + technical deep dives (memory, plugins, multi-agent) before building more demos.
        *Implication:* Fast, developer-friendly clarity, but may not convince skeptics without lived product experiences.
    c) Position as the best composable wrapper and embrace it; optimize for integrations and plugin breadth.
        *Implication:* Clear narrative and ecosystem pull, but risks diluting the “operating system” ambition and autonomy trajectory.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Scope Discipline: High-Frequency Solana Trading Demands vs. Platform Mission

**Summary of Topic:** Discord discussions surfaced an extreme-performance trading requirement set (ms GRPC ingestion, preshot payload systems, live X/Twitter token detection) that could distract from the monthly directive unless treated as an R&D track with strict boundaries.

#### Deliberation Items (Questions):

**Question 1:** Do we formally treat ultra-low-latency Solana trading as out-of-scope for core ElizaOS (and instead enable it via specialized community plugins), or do we invest in first-party infrastructure?

  **Context:**
  - `Discord 2026-01-04 (Chucknorris): "Need for GRPC ingesters with millisecond precision" and "Standard reaction-time agents would be too slow."`
  - `Discord 2026-01-04: "Jupiter or SDK implementations introduce ~4 second delays (too slow)."`

  **Multiple Choice Answers:**
    a) Out-of-scope for core: document constraints and provide extension points; encourage specialized external implementations.
        *Implication:* Protects execution excellence and broad developer UX, while still supporting power users through composability.
    b) Selective investment: build a first-party “low-latency ingestion” reference module as an optional advanced package.
        *Implication:* Creates a credible flagship for cross-chain/high-performance agents, but increases maintenance burden and risk.
    c) Full commitment: prioritize first-party Solana HFT-grade stack (GRPC ingest, preshot, bundlers) as a flagship.
        *Implication:* Could capture a high-visibility niche, but likely derails Cloud/stability priorities and narrows appeal.
    d) Other / More discussion needed / None of the above.

**Question 2:** If we support trading-adjacent use cases, what is the Council-approved “minimum viable truth” we can promise developers without misleading performance expectations?

  **Context:**
  - `Discord 2026-01-04: "Token creation monitoring requires live Twitter feeds and automated bundlers."`
  - `Discord 2026-01-04: "Implement GRPC ingester for processing DEX transactions" listed as an action item.`

  **Multiple Choice Answers:**
    a) Promise only event-driven automation at human/seconds-level latency; clearly warn that HFT is not supported.
        *Implication:* Preserves trust through accurate claims, but may disappoint a subset of speculative builders.
    b) Promise “near-real-time” with defined targets (e.g., sub-second ingest) for specific supported chains/providers.
        *Implication:* Sharper value proposition, but requires engineering and ongoing monitoring to keep targets truthful.
    c) Avoid latency promises; provide flexible architecture and let builders benchmark and choose tradeoffs.
        *Implication:* Safest legally/operationally, but can read as evasive and reduce adoption for performance-sensitive apps.
    d) Other / More discussion needed / None of the above.

**Question 3:** How should we route ongoing work like DegenAI and the Eliza knowledge data pipelines to strengthen the mission (Cloud + DX + reliability) rather than fragment attention?

  **Context:**
  - `Discord 2026-01-04: "Odlitime is continuing development work on DegenAI."`
  - `Discord 2026-01-04: "Jin is working on these [Eliza knowledge] pipelines and will soon begin documentation and presentation phases."`

  **Multiple Choice Answers:**
    a) Fold both into a unified “Flagship Reliability + Docs” program with clear deliverables tied to Cloud onboarding.
        *Implication:* Turns side projects into trust-building artifacts, strengthening developer-first outcomes.
    b) Keep them as separate labs tracks with explicit timeboxes and monthly demos; no impact on core priorities.
        *Implication:* Maintains innovation while protecting execution excellence, but risks weaker integration into product narrative.
    c) Pause or deprioritize non-core tracks until Cloud launch and stability milestones are met.
        *Implication:* Maximizes focus for near-term shipping, but may reduce community excitement and R&D momentum.
    d) Other / More discussion needed / None of the above.