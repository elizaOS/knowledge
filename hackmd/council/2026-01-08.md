# Council Briefing: 2026-01-08

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- A production-facing reliability breach surfaced as ElizaOS v1.7.0 broke Discord integrations via an incomplete serverId→messageServerId migration, forcing an urgent stabilization release path to protect developer trust.

## Key Points for Deliberation

### 1. Topic: v1.7.0 Discord Regression: Compatibility, Release Discipline, and Trust

**Summary of Topic:** A critical regression prevents Discord bots from resolving server IDs in v1.7.0 due to incomplete serverId→messageServerId migration, with fixes in PR #6333 and the odi-17 branch but requiring cross-branch plugin testing and a coordinated release.

#### Deliberation Items (Questions):

**Question 1:** What is the Council’s preferred immediate containment strategy for developer-facing Discord breakage in v1.7.0?

  **Context:**
  - `DigitalDiva (Discord): "No server ID found 10" on v1.7.0; bot fails to recognize server IDs.`
  - `Odilitime: advised downgrade to core v1.6.5 or try odi-17 while PR #6333 is tested/merged.`

  **Multiple Choice Answers:**
    a) Ship an emergency core patch release immediately with PR #6333 and publish a prominent downgrade workaround for Discord users.
        *Implication:* Fast trust repair, but risks shipping without full matrix testing and may create follow-up regressions.
    b) Hold core release until a full compatibility matrix passes across plugin-discord branches, and publish a temporary pinned advisory (use v1.6.5 / odi-17).
        *Implication:* Improves release quality signals, but prolongs broken experiences and may slow adoption of v1.7.x.
    c) Dual-track: immediate hotfix prerelease (beta tag) plus a scheduled stable release after automated integration tests are added.
        *Implication:* Balances urgency with rigor, but requires release-process overhead and clear comms to avoid confusion.
    d) Other / More discussion needed / None of the above.

**Question 2:** Should we treat the serverId→messageServerId transition as a breaking change requiring formal deprecation policy and migration tooling?

  **Context:**
  - `Odilitime: root cause is incomplete migration across codebase creating bootstrap/plugin-discord incompatibilities.`
  - `PR #6333: "serverId => messageServerId change" updates bootstrap actions/providers and SQL schema references.`

  **Multiple Choice Answers:**
    a) Yes—declare it breaking, add a deprecation window, provide codemods/linters, and version-gate plugins explicitly.
        *Implication:* Stronger long-term ecosystem stability; slower short-term iteration and more maintenance burden.
    b) No—keep it a silent internal migration and enforce backwards-compat fields in core types until plugins catch up.
        *Implication:* Minimizes friction now, but raises the chance of future hidden breakages and inconsistent semantics.
    c) Hybrid—treat as non-breaking in runtime (compat shim), but enforce strictness in dev tooling and docs moving forward.
        *Implication:* Preserves current users while pushing the ecosystem toward consistent APIs with minimal runtime disruption.
    d) Other / More discussion needed / None of the above.

**Question 3:** How should we harden our connector/plugin release pipeline to prevent “core release breaks major connector” events?

  **Context:**
  - `Odilitime: "additional testing across multiple Discord branches would be required before cutting a new Discord release."`
  - `core-devs: urgent release discussed; Discord plugin needs new release after testing.`

  **Multiple Choice Answers:**
    a) Add mandatory integration tests that run core + top connectors (Discord/Telegram) in CI before any core tag.
        *Implication:* Reduces regressions materially; increases CI time and maintenance for connector fixtures.
    b) Introduce compatibility contracts: core publishes a stable messaging/room schema interface and connectors must pin to it.
        *Implication:* Improves composability and predictability, but requires upfront specification work and enforcement.
    c) Adopt staged releases: canary deployments and prerelease tags for core/connector pairs before stable promotion.
        *Implication:* Improves real-world validation, but needs operational maturity and clear version signaling to developers.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Cloud Reliability and Scale: TOCTOU Fixes + Event Pump Architecture

**Summary of Topic:** Cloud work is addressing TOCTOU credit-deduction race conditions and runtime initialization performance; in parallel, core-devs are converging on a scaling model of simple event pumps with multiple daemon instances and differentiated QoS for voice vs text connectors.

#### Deliberation Items (Questions):

**Question 1:** What reliability guarantees should ElizaOS Cloud enforce for credit/accounting under streaming and concurrent requests?

  **Context:**
  - `Stan standup: TOCTOU race conditions fixed via "deduct-before, reconcile-after" approach; Linear tickets created.`
  - `core-devs: runtime initialization optimizations underway.`

  **Multiple Choice Answers:**
    a) Strong consistency: never allow negative/overdraw; fail fast when credits are uncertain.
        *Implication:* Maximizes trust and predictability, but may reduce UX smoothness during transient failures.
    b) Eventual consistency with reconciliation: allow short-lived discrepancies with automated correction and audit logs.
        *Implication:* Improves availability and throughput, but requires strong observability to prevent perceived billing unfairness.
    c) Tiered consistency: strong for paid/prod tenants, relaxed for free/dev tiers to optimize cost.
        *Implication:* Aligns reliability with revenue and cost, but introduces complexity and potential confusion across tiers.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should we formalize the connector scaling model (event pumps) to support both text and voice workloads without fragmenting architecture?

  **Context:**
  - `Odilitime: "Direction is simple event pumps" and "multiple daemon instances per service due to scale."`
  - `Odilitime: "Voice connections require higher priority/bandwidth event pumps than text; preprocessing valuable."`

  **Multiple Choice Answers:**
    a) Single unified pump type with priority queues and QoS controls (voice gets higher priority lanes).
        *Implication:* Keeps architecture simple and composable, but demands careful QoS tuning and backpressure design.
    b) Separate pump classes (voice-pump vs text-pump) with different SLAs and resource profiles.
        *Implication:* Optimizes each workload, but risks duplicated logic and connector divergence over time.
    c) Gateway-per-connector with standardized pump interface underneath (each connector owns its gateway).
        *Implication:* Speeds connector iteration and isolates failures, but increases operational surface area and coordination cost.
    d) Other / More discussion needed / None of the above.

**Question 3:** Which reference implementation should become the canonical pattern for Discord bridging in Cloud: Jeju branch now, or a new consolidated design?

  **Context:**
  - `Odilitime: recommended reviewing Jeju cloud branch with Shaw's preferred Discord bridge (eliza-cloud-v2/jeju/apps/discord-gateway).`
  - `core-devs: connector gateway architecture discussion tied to scaling strategy.`

  **Multiple Choice Answers:**
    a) Adopt Jeju as canonical immediately and iterate in-place to converge the ecosystem faster.
        *Implication:* Accelerates standardization, but may lock in design tradeoffs before broader review.
    b) Create an RFC-driven consolidated design, using Jeju as one input among multiple prototypes.
        *Implication:* Improves correctness and buy-in, but delays shipping and prolongs architectural uncertainty.
    c) Maintain Jeju as experimental; focus first on a minimal event-pump spec and compliance tests for any bridge.
        *Implication:* Enforces interoperability while allowing innovation, but requires discipline to prevent endless divergence.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Developer Experience and Public Trust: Documentation, Discoverability, and “Known Answers”

**Summary of Topic:** Repeated support incidents reveal documentation gaps (Cloud model naming, destructive migrations, command choice) and community trust friction (contract address discoverability), indicating an urgent need to centralize “known answers” and improve official surface area without appearing scam-like.

#### Deliberation Items (Questions):

**Question 1:** Where should the official contract address be surfaced to maximize safety and discoverability without triggering “scam optics”?

  **Context:**
  - `Broccolex/community: difficulty finding official ElizaOS CA on X; wants discoverability within 10 seconds.`
  - `Shaw: committed to improving CA visibility; discussion noted "CAs in bios look scammy" (daily summary).`

  **Multiple Choice Answers:**
    a) Pin an official X post (and mirror on website) with CA + verification guidance; keep bios clean.
        *Implication:* Balances legitimacy and discoverability while reducing spoofing risk via a single canonical message.
    b) Place CA directly in all official account bios plus website header for maximal visibility.
        *Implication:* Fastest for users, but increases scam-like perception and may normalize unsafe behavior patterns.
    c) Use CoinGecko/CoinMarketCap as canonical references via Linktree and website, avoiding direct CA posting except in docs.
        *Implication:* Leverages third-party verification, but can slow user lookup and creates dependency on external listings.
    d) Other / More discussion needed / None of the above.

**Question 2:** What “first-run” documentation must be elevated to reduce recurring integration failures for Cloud and local dev?

  **Context:**
  - `cjft: model parameter must use provider prefix (e.g., "openai/gpt-4o-mini").`
  - `Andrei: destructive migration blocked; fix via ELIZA_ALLOW_DESTRUCTIVE_MIGRATIONS=true; Omid: use elizaos dev for continuous monitoring.`

  **Multiple Choice Answers:**
    a) Publish a single “First 30 Minutes” guide: Cloud API calling conventions, model naming, and CLI dev/start differences.
        *Implication:* Reduces support load and improves activation rate with a unified onboarding narrative.
    b) Add inline CLI and API error messages that link to targeted docs sections (docs-as-runtime guidance).
        *Implication:* Fixes issues at the moment of failure, but requires careful versioning and link stability.
    c) Rely on community Q&A and periodic summaries; prioritize code changes over docs updates.
        *Implication:* Maximizes engineering throughput short-term, but increases repeated friction and erodes “developer-first” credibility.
    d) Other / More discussion needed / None of the above.

**Question 3:** How should we operationalize “Taming Information” so that Discord-resolved answers become durable, searchable canon?

  **Context:**
  - `jin shared HackMD book workspace (https://hackmd.io/@elizaos/book) and GitHub agentics workflow patterns.`
  - `Daily incidents show repeated Q&A resolving the same issues (model naming, migration flags, Discord plugin breakage).`

  **Multiple Choice Answers:**
    a) Mandate that any resolved support incident over a threshold (e.g., >2 occurrences) triggers a docs PR within 48 hours.
        *Implication:* Creates a tight feedback loop that steadily reduces support burden and increases trust through shipping.
    b) Deploy an “Answer Harvester” agent that auto-extracts Q&A from Discord into draft docs, reviewed weekly by maintainers.
        *Implication:* Scales documentation without blocking engineers, but needs governance to prevent inaccuracies from being canonized.
    c) Keep summaries only (daily/weekly), and accept that documentation lags behind fast-moving development.
        *Implication:* Lower process overhead, but conflicts with execution excellence and leads to repeated integration failures.
    d) Other / More discussion needed / None of the above.