# Council Briefing: 2026-01-01

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- Operational momentum is concentrated on launch sequencing (Cloud/Babylon → Jeju public readiness) while trust risks accumulate from migration UX friction, a broken OAuth3 demo login, and a brief lull in visible GitHub shipping.

## Key Points for Deliberation

### 1. Topic: Cloud/Babylon Launch Sequencing & Jeju Infrastructure Readiness

**Summary of Topic:** Core signals indicate a push to have infrastructure “done and tested” on Jeju before going public, but the Jeju OAuth3 testnet’s demo login is currently failing—an execution-excellence risk if this becomes a first-impression surface during Cloud/Babylon launch.

#### Deliberation Items (Questions):

**Question 1:** What is the Council’s preferred launch order to maximize developer trust: Cloud first, Babylon first, or a synchronized dual launch with Jeju readiness gates?

  **Context:**
  - `Shaw (core-devs/partners): migrating infrastructure to run on “jeju” in preparation for going public after launching “cloud” and “babylon” products.`

  **Multiple Choice Answers:**
    a) Cloud-first, Babylon follows after stability SLOs are met; Jeju public positioning comes last.
        *Implication:* Maximizes DX and reliability narrative, but delays the broader ecosystem/token story tied to Babylon/Jeju.
    b) Babylon-first to anchor ecosystem incentives; Cloud rolls out as the execution layer immediately after.
        *Implication:* Optimizes incentive flywheel, but increases reputational risk if developer UX is not already smooth.
    c) Dual launch (Cloud + Babylon) with explicit Jeju readiness gates and a single coordinated public milestone.
        *Implication:* Creates a powerful “platform moment,” but requires unusually high operational coordination and zero-tolerance for demo failures.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should we treat the failing OAuth3 demo login: block external promotion until fixed, ship with limitations and a banner, or replace with a simplified auth demo?

  **Context:**
  - `Shaw (core-devs): shared OAuth3 testnet site “oauth3.testnet.jejunetwork.org”… routed through an on-chain registry to an IPFS asset.`
  - `Odilitime (core-devs): “Does the demo login work…? It doesn’t work, but otherwise looks good.”`

  **Multiple Choice Answers:**
    a) Hard gate: no public amplification until demo login works end-to-end.
        *Implication:* Protects trust-through-shipping, but may slow momentum and partner excitement.
    b) Soft ship: keep it live with a prominent limitation banner and a public ETA for the fix.
        *Implication:* Preserves velocity while managing expectations, but still risks “paper-cut” first impressions.
    c) Swap demo: replace login with a minimal, deterministic auth flow (or read-only showcase) until OAuth3 is stable.
        *Implication:* Reduces risk immediately while preserving narrative, but may create rework and confusion about “real” readiness.
    d) Other / More discussion needed / None of the above.

**Question 3:** What infrastructure posture best serves execution excellence in 2026: remain on AWS through launch, hybridize early, or accelerate toward self-owned permissionless racks?

  **Context:**
  - `Shaw (Dec 30): “initially launch on AWS with a goal to transition to self-owned permissionless infrastructure… by year-end.”`

  **Multiple Choice Answers:**
    a) AWS-first through launch + first 90 days, then transition with measured SLOs.
        *Implication:* Prioritizes reliability and ops simplicity during the trust-building window.
    b) Hybrid early: core control-plane on AWS, selective workloads on Jeju/self-owned infra ASAP.
        *Implication:* Demonstrates decentralization cred without betting the launch on new ops complexity.
    c) Accelerate self-owned racks before/at launch to signal permissionless intent strongly.
        *Implication:* Maximizes narrative alignment, but increases outage risk and operational burden during the most visible period.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Token Migration Reliability, Wallet Friction, and Gated Access Operations

**Summary of Topic:** Migration urgency remains high (ends February) and community demand is strong, but recurring wallet-transfer/connectivity issues plus unclear token utility messaging threaten the “reliable, developer-friendly” trust contract if not operationally systematized.

#### Deliberation Items (Questions):

**Question 1:** What is the Council’s minimum acceptable migration success standard: prioritize reducing user confusion, reducing wallet friction, or accelerating deadline-driven completion?

  **Context:**
  - `Discord (Dec 31): “migration ends in February.”`
  - `Multiple users asking migration/contract/bridge questions; some unanswered in chat analysis.`

  **Multiple Choice Answers:**
    a) Confusion-first: consolidate canonical migration docs/FAQ, pin contract addresses, and reduce contradictory answers.
        *Implication:* Improves trust and reduces support load, but may delay feature work short-term.
    b) Friction-first: prioritize wallet/connect fixes and step-by-step flows over new messaging content.
        *Implication:* Directly increases completion rate, but may leave narrative gaps around utility and ecosystem structure.
    c) Deadline-first: focus on throughput (support staffing + automation) to finish migration on schedule, accept some UX debt.
        *Implication:* Protects timeline optics, but risks long-term reputational damage and developer churn.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should we operationalize wallet support given recurring MetaMask/Phantom issues: official wallet guidance, tooling changes, or outsourced support playbooks?

  **Context:**
  - `Kenk (Dec 31): suggested importing MetaMask EOA into Phantom due to MetaMask issues.`
  - `InvB/jasyn_bjorn (Dec 31): Phantom wallet connection issues on desktop troubleshooting.`

  **Multiple Choice Answers:**
    a) Publish an official “supported wallets & known issues” matrix with recommended paths (MetaMask → Phantom EOA import, etc.).
        *Implication:* Reduces repeated questions and increases completion rate, strengthening execution excellence.
    b) Invest in tooling/UX: build or integrate a guided migration checker that detects wallet state and offers fixes.
        *Implication:* Higher engineering cost, but creates a scalable trust asset and reduces human support dependency.
    c) Scale support ops: standardized scripts + volunteer/mod training + ticket routing; minimal product changes.
        *Implication:* Fast to deploy, but may not reduce underlying friction and could burn out community helpers.
    d) Other / More discussion needed / None of the above.

**Question 3:** What is the Council’s policy on token-gated programs (Spartan group, 1M-token autotrader access) during migration: expand gating, pause gating, or keep but clarify aggressively?

  **Context:**
  - `Dec 30: multiple users asked to join Spartan group; Kenk: verify holding in #verify-wallet.`
  - `Dec 29/31: DegenAI autotrader access for holders of 1M+ tokens; ecosystem token relationship questions persist.`

  **Multiple Choice Answers:**
    a) Expand gating: use access tiers to drive ecosystem engagement and reward committed holders.
        *Implication:* May strengthen token narrative, but increases fairness/complexity scrutiny during migration.
    b) Pause gating until migration stabilizes and docs are unambiguous.
        *Implication:* Reduces confusion and support burden, but may slow community growth dynamics and partner excitement.
    c) Keep gating but clarify: publish a single canonical page explaining tiers, verification, benefits, and risks/limits.
        *Implication:* Preserves incentives while aligning with “developer-first” clarity and operational trust.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Trust Through Shipping: Cadence, Release Automation, and Information-Taming Signals

**Summary of Topic:** While late-December delivered meaningful stability and plugin improvements, the Jan 1 GitHub signal is near-silent and release/versioning friction is emerging—counterbalanced by promising “taming information” work (Jin’s activity-to-art pipeline) that could strengthen narrative and documentation velocity if productized.

#### Deliberation Items (Questions):

**Question 1:** How should the Council respond to the visible dip in core repo activity: treat as normal holiday variance, or institute a launch-critical shipping cadence with explicit weekly deliverables?

  **Context:**
  - `GitHub daily summary (Dec 31–Jan 1): “minimal activity… no new pull requests created or merged… 2 active contributors.”`

  **Multiple Choice Answers:**
    a) Accept variance: no policy change; focus on upcoming launch milestones rather than daily optics.
        *Implication:* Avoids process overhead, but risks perception of stalled execution during a sensitive trust window.
    b) Institute a short-term “launch cadence” mandate: weekly targets, ship notes, and visible status dashboards.
        *Implication:* Improves trust-through-shipping and coordination, but requires disciplined PM/eng bandwidth.
    c) Rebalance resourcing: temporarily shift effort from side initiatives to core Cloud/migration reliability work.
        *Implication:* Maximizes execution excellence, but may reduce exploratory innovation and contributor enthusiasm.
    d) Other / More discussion needed / None of the above.

**Question 2:** What release process best supports Developer First: keep manual version bumps per plugin PR, adopt monorepo-style automated releases, or enforce fewer, bundled releases?

  **Context:**
  - `Stan (Dec 30): “Should I pump the version for every plugin PR… we should have a CI… like release please or something like that.”`

  **Multiple Choice Answers:**
    a) Automate releases (release-please/changesets) across plugins with consistent semantic versioning.
        *Implication:* Reduces maintainer burden and improves DX predictability, strengthening ecosystem trust.
    b) Keep manual bumps but formalize rules (when to bump, who approves) and add CI checks.
        *Implication:* Lower tooling complexity, but still leaves room for human error and inconsistency.
    c) Bundle releases on a fixed cadence (e.g., weekly) to reduce churn and coordination overhead.
        *Implication:* Improves stability perception, but slows delivery of urgent fixes and can frustrate contributors.
    d) Other / More discussion needed / None of the above.

**Question 3:** Do we elevate Jin’s “activity-to-newspaper art” pipeline into an official Information-Taming surface for Cloud/Framework (dashboards, release notes, council briefs), or keep it experimental?

  **Context:**
  - `Jin (Dec 31 coders): “image pipeline… generates art based on summarized activities from Discord and GitHub… seasonal variance… newspaper-style images.”`

  **Multiple Choice Answers:**
    a) Productize now: integrate into official comms (weekly recaps, release notes, Cloud changelog visuals).
        *Implication:* Strengthens narrative and community trust signals, but requires editorial control and reliability guarantees.
    b) Keep experimental: treat as optional community artifact until core launch stability is achieved.
        *Implication:* Avoids distraction, but may miss a leverage point for “trust through shipping” and community delight.
    c) Hybrid: formalize the data pipeline (structured summaries) but keep image generation as a non-critical add-on.
        *Implication:* Advances Information-Taming reliability while limiting risk from generative output quality/consistency.
    d) Other / More discussion needed / None of the above.