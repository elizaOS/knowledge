# Council Briefing: 2025-01-03

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- Developer trust is being won or lost on reliability: rapid GitHub shipping continues, but recurring Twitter/client and install-time failures are creating frontline friction that needs decisive stabilization.

## Key Points for Deliberation

### 1. Topic: Reliability Front: Twitter Client + Install/DB Friction

**Summary of Topic:** Community demand is bottlenecked by practical failures (Twitter login/rate limiting/duplicate replies, SQLite/Postgres errors, Windows dev server issues). Recent fixes landed, but the Council must choose whether to harden the platform via an explicit stabilization program rather than continuing feature expansion as the dominant cadence.

#### Deliberation Items (Questions):

**Question 1:** Do we declare a short-term stabilization freeze on new integrations until Twitter + database + Windows setup issues fall below an agreed failure threshold?

  **Context:**
  - `Discord (2025-01-02): "Multiple users reported issues with the Twitter client, including login failures, rate limiting, and duplicate replies to tweets."`
  - `GitHub Daily (2025-01-03): "Improved Windows compatibility for the Vite development server" (PR #1760) and "Reported issues regarding slow startup times with pnpm" (#1758).`

  **Multiple Choice Answers:**
    a) Yes—announce a 2-week stabilization sprint with a hard bar for merge acceptance (tests/telemetry/docs) on core clients.
        *Implication:* Signals execution excellence and reduces churn, but may slow ecosystem plugin velocity temporarily.
    b) Partial—freeze only the Twitter surface area (client + plugin) while other integrations proceed.
        *Implication:* Targets the highest reputational risk while preserving momentum elsewhere, but risks shifting instability to other layers.
    c) No—maintain current velocity and rely on opportunistic fixes from the community.
        *Implication:* Maximizes feature output, but compounds support load and can erode developer trust if core paths remain unreliable.
    d) Other / More discussion needed / None of the above.

**Question 2:** What is our canonical "default" storage path for new developers (SQLite vs Postgres/Supabase), and do we enforce it via tooling/templates?

  **Context:**
  - `Discord (2025-01-02): "Mike D. shared a fixed PR for SQLite installation errors that many users were encountering."`
  - `GitHub Issues (2025-01-02 summary): "Users are facing issues starting the agent with PostgreSQL (#1687)."`

  **Multiple Choice Answers:**
    a) Default to SQLite (lowest friction) and treat Postgres/Supabase as documented opt-ins with strong migration guides.
        *Implication:* Improves onboarding success rate quickly, but may limit scale/perf assumptions for production users.
    b) Default to Postgres/Supabase for production realism; invest in flawless setup scripts and health checks.
        *Implication:* Aligns with Cloud readiness and serious deployments, but raises the barrier for first-time builders.
    c) Support both equally and avoid prescribing; let templates and community guides compete.
        *Implication:* Avoids contentious standardization, but prolongs confusion and multiplies documentation/support burden.
    d) Other / More discussion needed / None of the above.

**Question 3:** Do we make observability (OpenTelemetry/logging standards) a first-class requirement for core clients to accelerate debugging and reduce duplicate bug reports?

  **Context:**
  - `Discord (2025-01-02): "Implement OpenTelemetry for better debugging and tracing (Mentioned by Mike D.)."`
  - `GitHub Daily (2025-01-03): "Implemented logging capabilities for the eternalai provider" (PR #1740) and "replace console.log with Eliza logger" (PR #1745).`

  **Multiple Choice Answers:**
    a) Yes—mandate OTel hooks and structured logging for Twitter/Discord/DB adapters before the next minor release.
        *Implication:* Reduces MTTR and increases reliability, but adds short-term engineering overhead and review complexity.
    b) Incremental—ship OTel as an optional plugin and only require it in Cloud deployments.
        *Implication:* Balances effort with impact, but leaves self-hosted debugging inconsistent.
    c) No—prioritize fixes without instrumentation; rely on logs and community reproduction steps.
        *Implication:* Speeds near-term shipping, but keeps root-cause analysis slow and repetitive at scale.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Throughput vs Governance: Managing High-Volume Contribution Streams

**Summary of Topic:** Repo activity is surging (dozens of PRs per day, many contributors), while the merge ratio dropped sharply on the latest day—an early warning that review bandwidth and CI gates may become the limiting factor. The Council must decide how to maintain quality (Execution Excellence) amid rapid expansion.

#### Deliberation Items (Questions):

**Question 1:** Should we shift to a stricter merge policy (tests + docs + risk notes) for core packages to prevent quality regression as PR volume accelerates?

  **Context:**
  - `GitHub Activity Update: "31 new PRs (18 merged)… then 43 new PRs (14 merged)… merged PRs decreased from ~58% to ~33%."`

  **Multiple Choice Answers:**
    a) Yes—raise the bar on core packages (agent runtime, clients, adapters) while keeping plugins more permissive.
        *Implication:* Protects reliability in the critical path while still enabling ecosystem growth at the edges.
    b) Yes—uniformly enforce stricter gates across the whole monorepo to reduce long-tail breakage.
        *Implication:* Improves overall quality, but risks discouraging casual contributors and slowing plugin innovation.
    c) No—prioritize merge velocity and handle regressions post-merge with rapid patch releases.
        *Implication:* Maintains momentum, but increases user-facing instability and support burden (trust risk).
    d) Other / More discussion needed / None of the above.

**Question 2:** How do we operationalize "open & composable" without letting plugin sprawl degrade the default experience for new developers?

  **Context:**
  - `Discord (2025-01-01): "The community is documenting 45+ plugins available in ElizaOS" (0xwitch).`
  - `GitHub (month aggregate): "1039 new PRs (735 merged)… 694 active contributors."`

  **Multiple Choice Answers:**
    a) Define a curated "core distribution" (blessed plugins + defaults) and move experimental plugins to a separate channel/registry tier.
        *Implication:* Creates a reliable default DX while preserving experimentation, at the cost of maintaining a clear curation process.
    b) Keep everything in one place but improve discoverability (plugin taxonomy, quality badges, compatibility matrices).
        *Implication:* Preserves openness, but still risks overwhelming new users and complicating support.
    c) Aggressively prune/deprecate low-quality plugins from the mainline.
        *Implication:* Raises baseline quality quickly but can alienate contributors and reduce ecosystem breadth.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Information Command & Tokenomics Comms: Trust Through Clarity

**Summary of Topic:** Two trust vectors are converging: (1) building an agentic project manager to route issues/milestones and tame scattered information, and (2) urgent clarity around ecosystem entry fees/launchpad expectations. Both are governance-by-shipping problems: the platform must be legible, not just capable.

#### Deliberation Items (Questions):

**Question 1:** Do we treat the "Eliza agent project manager" as a flagship internal product (first-class roadmap + funding) to operationalize Taming Information, or keep it experimental?

  **Context:**
  - `Discord (2025-01-02): "Jin mentioned working on funding… to develop an Eliza agent project manager that will track issues, milestones, and route information."`

  **Multiple Choice Answers:**
    a) Flagship it—fund and ship an MVP that integrates Discord+GitHub summaries, assigns owners, and generates weekly council briefs.
        *Implication:* Directly advances Execution Excellence and information hygiene, accelerating all other initiatives.
    b) Keep it experimental—support community prototypes and only formalize once usage proves value.
        *Implication:* Reduces upfront cost and risk, but delays the benefits of unified coordination.
    c) Outsource to a partner/bounty program and standardize interfaces rather than building in-house.
        *Implication:* Speeds development via parallelism, but risks fragmentation and uneven quality control.
    d) Other / More discussion needed / None of the above.

**Question 2:** How do we enforce and communicate ecosystem entry rules (e.g., 5–10% launchpad fee) without triggering backlash or forks that dilute the brand?

  **Context:**
  - `Discord tokenomics (2025-01-02): "DorianD suggested urgent implementation of clear communication about the 5-10% ecosystem entry fee… Odilitime agreeing to update the repository README."`

  **Multiple Choice Answers:**
    a) Publish an official policy + clickwrap in token creation flows; make participation explicit and opt-in with clear benefits.
        *Implication:* Maximizes clarity and legitimacy, but requires product/legal alignment and disciplined enforcement.
    b) Soft enforcement: document it in README/docs and rely on social enforcement + partner relationships.
        *Implication:* Lower friction and faster, but may not stop misunderstandings and ecosystem drift.
    c) Remove/relax the fee model and instead capture value via optional premium services (Cloud, support, distribution).
        *Implication:* Reduces rebellion risk, but changes treasury/value-capture assumptions and may require new monetization rails.
    d) Other / More discussion needed / None of the above.

**Question 3:** Do we prioritize "source citation" and "sanitized LLM feed" as trust primitives for flagship agents before expanding into more ambitious media formats (e.g., AI TV/news show)?

  **Context:**
  - `Discord (2025-01-02): "Develop sanitized LLM feed to reduce hallucinations…" (Hunter).`
  - `Partners (2025-01-02): "Implement source citation capability for agents to increase believability" (avirtualfuture).`

  **Multiple Choice Answers:**
    a) Yes—treat citations + anti-hallucination pipelines as gating requirements for flagship/public-facing agents.
        *Implication:* Improves credibility with developers and normies, strengthening the brand as 'reliable agents' rather than flashy demos.
    b) Split-track—ship media experiments while building trust primitives in parallel.
        *Implication:* Maintains momentum and community excitement, but risks reputational damage if public demos hallucinate.
    c) No—focus on content velocity and treat trust improvements as post-MVP polish.
        *Implication:* Accelerates narrative output, but undermines the Council’s stated principle of Execution Excellence.
    d) Other / More discussion needed / None of the above.