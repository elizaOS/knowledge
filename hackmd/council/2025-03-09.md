# Council Briefing: 2025-03-09

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- A critical build-pipeline fracture was sealed (Rollup external regression), but the day exposed a broader readiness risk: installation and Twitter/plugin regressions are now the primary threat to “trust through shipping.”

## Key Points for Deliberation

### 1. Topic: Release & Build Stability (Trust Through Shipping)

**Summary of Topic:** The org shipped a targeted build fix (Rollup external) while fresh issues surfaced around pnpm installs, plugin interactions, and tutorial correctness—indicating stability work must outrank net-new capability until onboarding is reliably reproducible.

#### Deliberation Items (Questions):

**Question 1:** Should the Council impose a release gate that blocks v2 launch and Cloud launch announcements until “clean install + start” succeeds across a defined matrix (OS, package manager, starter vs main repo)?

  **Context:**
  - `GitHub Daily (2025-03-09): “Implemented a fix for a missing moment rollup external… resolving issues with the-org in elizaos/eliza#3876.”`
  - `GitHub Daily (2025-03-09): Issue #3882: “Issues with pnpm install and build after switching from eliza-starter to the main eliza repository.”`

  **Multiple Choice Answers:**
    a) Yes—introduce a hard release gate with a public compatibility matrix and required green CI artifacts.
        *Implication:* Builds external trust and reduces support load, but may delay feature timelines and marketing beats.
    b) Partially—gate only Cloud/v2 “stable” labels, but allow preview/alpha releases to ship continuously.
        *Implication:* Maintains momentum while signaling risk boundaries clearly; requires strong labeling discipline.
    c) No—optimize for velocity; rely on rapid patches and community troubleshooting.
        *Implication:* Short-term throughput increases, but repeated breakages compound reputational damage and onboarding churn.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should we manage dependency churn (Renovate, large upgrade waves) to prevent “silent destabilization” of core workflows?

  **Context:**
  - `Daily Summary (2025-03-08): “numerous dependency updates… Solana packages… typescript-eslint… PNPM… langchain…”`
  - `GitHub Activity (2025-03-08 to 2025-03-09): “72 new PRs (36 merged)… 35 active contributors.”`

  **Multiple Choice Answers:**
    a) Adopt a “stability train”: batch dependency updates into scheduled windows with mandatory install/build smoke tests.
        *Implication:* Reduces regression frequency and improves predictability, at the cost of slower CVE/upgrade ingestion.
    b) Allow continuous dependency merges, but require an automated “golden path” e2e test per Renovate PR.
        *Implication:* Preserves velocity while defending the core UX, but demands investment in robust e2e infrastructure.
    c) Freeze dependencies until v2 ships, except critical security fixes.
        *Implication:* Stabilizes near-term releases, but accumulates technical debt and may increase future integration risk.
    d) Other / More discussion needed / None of the above.

**Question 3:** What is the Council’s stance on large, high-diff PRs (10k–100k+ line changes) that may conceal regressions despite good intent?

  **Context:**
  - `GitHub Weekly Contributor Notes (week of 2025-03-09): “PR #3887… updated Docker files… +102,949/-52,621 lines.”`
  - `GitHub Top PRs (month view): “PR #3920 ‘Gaia’… additions 538,730; deletions 5,518 (unmerged).”`

  **Multiple Choice Answers:**
    a) Enforce “segmented merges”: require large PRs to be split into reviewable, testable slices before merge.
        *Implication:* Improves review quality and reduces risk, but increases contributor friction and coordination overhead.
    b) Permit large PRs only with enhanced controls (mandatory design doc, pair review, and targeted e2e suite).
        *Implication:* Balances throughput with safeguards, but relies on consistent reviewer availability and process adherence.
    c) Continue current approach; accept occasional regressions as the price of rapid evolution.
        *Implication:* Maximizes merge velocity, but increases the probability of trust-eroding breakages in core workflows.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Developer Onboarding & Plugin UX (DX as a Force Multiplier)

**Summary of Topic:** Discord support traffic indicates users are stumbling on plugin installation changes, Twitter client behavior, and knowledge/RAG workflows; this is a direct contradiction to “Developer First” unless the CLI/docs become a single coherent runway.

#### Deliberation Items (Questions):

**Question 1:** Which “golden path” should be canon for new builders: starter repo, main repo, or Cloud-first CLI—given current confusion and breakage reports?

  **Context:**
  - `Discord (2025-03-07): “V2… easier agent creation with commands like `npx elizaos init` or through a GUI.”`
  - `GitHub Daily (2025-03-09): Issue #3882: “pnpm install and build after switching from eliza-starter to the main eliza repository.”`

  **Multiple Choice Answers:**
    a) Make Cloud-first CLI the default runway; treat local as an advanced path.
        *Implication:* Improves reliability and supportability, but may alienate local-first/open-source purists without careful messaging.
    b) Maintain two runways (starter + main), but formally version and document them with strict compatibility guarantees.
        *Implication:* Respects diverse workflows, but requires ongoing doc discipline and additional CI matrix cost.
    c) Consolidate onto the main repo only; deprecate starter except as archived examples.
        *Implication:* Reduces fragmentation, but increases onboarding burden if main remains heavier and more failure-prone.
    d) Other / More discussion needed / None of the above.

**Question 2:** Should we prioritize a “Twitter/Discord/TG reliability strike team” over new integrations (LinkedIn, Instagram, new chains) until core social clients behave predictably?

  **Context:**
  - `Discord coders (2025-03-08): “Twitter client integration problems… ENABLE_TWITTER_POST_GENERATION… min/max posting time.”`
  - `GitHub Daily (2025-03-09): Issue #3877: “Error processing tweet undefined after installing multiple plugins.”`

  **Multiple Choice Answers:**
    a) Yes—pause new client integrations; focus on reliability, test coverage, and configuration ergonomics for top 3 clients.
        *Implication:* Directly improves perceived quality and reduces churn, accelerating ecosystem trust and adoption.
    b) Hybrid—continue new integrations, but require each to ship with standardized docs, smoke tests, and templates.
        *Implication:* Keeps ecosystem growth narrative alive, while partially containing instability through process.
    c) No—ship breadth; let community plugins mature organically while core team focuses on v2 architecture.
        *Implication:* Increases feature surface rapidly, but risks eroding the “reliable framework” brand if flagship clients stay brittle.
    d) Other / More discussion needed / None of the above.

**Question 3:** What is the minimum documentation artifact we should treat as a ship-blocker to satisfy “Trust Through Shipping” (especially after doc URL migrations and CLI changes)?

  **Context:**
  - `Discord (2025-03-07): “old documentation site… no longer available, with the new docs at elizaos.github.io/eliza/docs.”`
  - `Discord coders (2025-03-08): “Plugin installation changed… using the ElizaOS CLI… caused confusion for users familiar with the old method.”`

  **Multiple Choice Answers:**
    a) A single “Getting Started” page that is CI-validated end-to-end (copy/paste runnable) plus an always-current plugin install guide.
        *Implication:* Converts support load into self-serve success, strengthening developer trust measurably.
    b) A docs versioning system and changelog discipline, but accept occasional mismatches as the project evolves fast.
        *Implication:* Improves clarity over time, but near-term onboarding pain persists without enforced runnable guarantees.
    c) Documentation is best-effort; prioritize code and rely on Discord/agents for guidance.
        *Implication:* Increases velocity short-term, but scales poorly and undermines the developer-friendly mission.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: V2 Readiness: Modular Routing, Unified Memory, and Identity

**Summary of Topic:** V2’s promise—modular clients, cross-platform routing, and unified memory—directly advances the North Star, but the Council must decide what “April launch” means: a stable core with migration path, or a broad feature-complete release.

#### Deliberation Items (Questions):

**Question 1:** What constitutes “v2 launch” in Council terms: stable routing + unified memory core, or full parity with v1 clients/actions/evaluators ecosystem?

  **Context:**
  - `Discord (2025-03-08, HoneyBadger): “elizaOS v2… expected to launch by April.”`
  - `Discord (2025-03-08, jintern): “V2… solves cross-platform message routing… allowing messages to flow automatically between platforms and fixing wallet confusion issues.”`

  **Multiple Choice Answers:**
    a) Define launch as “core stable”: routing + unified memory + reference agents; accept partial client parity.
        *Implication:* Delivers the key architectural leap sooner, but requires careful expectation setting for missing edges.
    b) Define launch as “ecosystem parity”: do not announce v2 until major clients and migration tooling are production-ready.
        *Implication:* Maximizes trust at announcement time, but risks timeline slips and narrative stagnation.
    c) Define launch as “developer preview”: ship quickly, iterate publicly, and treat stability as a post-launch campaign.
        *Implication:* Accelerates feedback loops, but can conflict with the execution-excellence directive if preview is perceived as unstable.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should unified memory behave across platforms: strict shared state, partitioned state with controlled bridges, or policy-driven per-client memory with reconciliation?

  **Context:**
  - `Discord (2025-03-07): “V2… implements unified agent memory across platforms while respecting platform-specific rules.”`
  - `Discord (2025-03-08, partners): Example cited: cross-platform visibility for DAO actions (Discord votes → Telegram visibility).`

  **Multiple Choice Answers:**
    a) Strict shared memory: one canonical timeline across all clients.
        *Implication:* Simplifies reasoning and interoperability, but increases privacy/rule-conflict risk across platforms.
    b) Partitioned memory with explicit bridges: each platform has its own store, bridged via routable events.
        *Implication:* Improves compliance and safety, but increases developer complexity and potential for desync.
    c) Policy-driven hybrid: default shared memory with per-client “memory scopes” and reconciliation rules.
        *Implication:* Balances safety and usability, but demands strong tooling and clear mental models in docs.
    d) Other / More discussion needed / None of the above.

**Question 3:** Should agent identity verification (TEE/ZK) be pursued as a near-term differentiator for v2/Cloud, or deferred until core runtime reliability is proven?

  **Context:**
  - `Discord (2025-03-08): “Agent Identity Verification… using TEE and zero-knowledge proofs.”`
  - `Discord (2025-03-08): “Governance Simulation… model different governance approaches for DAOs.”`

  **Multiple Choice Answers:**
    a) Prioritize now—ship a minimal verification spec/prototype alongside v2 to lead in trustable agents.
        *Implication:* Positions ElizaOS as a trust layer for autonomous systems, but may distract from fixing core DX/reliability pain.
    b) Stage it—define the architecture and interfaces now, but implement after v2 stability and Cloud onboarding harden.
        *Implication:* Keeps the trust narrative alive while protecting execution excellence on core deliverables.
    c) Defer—focus exclusively on routing/memory/runtime stability; revisit verification after adoption inflection.
        *Implication:* Maximizes near-term reliability, but risks losing differentiation in the “verifiable agent” arena.
    d) Other / More discussion needed / None of the above.