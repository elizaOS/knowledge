# Council Briefing: 2026-01-04

## Monthly Goal

December 2025: Execution excellence—complete token migration with high success rate, launch ElizaOS Cloud, stabilize flagship agents, and build developer trust through reliability and clear documentation.

## Daily Focus

- Primary signal: reliability and operability are improving (provider performance, logging standards, critical SQL fix), while new friction points emerge in Cloud/public-agent onboarding UX that could erode developer trust if not triaged quickly.

## Key Points for Deliberation

### 1. Topic: Execution Excellence: Reliability, Performance, and Observability Hardening

**Summary of Topic:** Core work continues to align with “Execution Excellence” via faster multi-provider execution, standardized logging (with lint enforcement), and a critical SQL plugin fix that restores database correctness under data isolation—each directly impacting trust and uptime for Cloud and persistent agents.

#### Deliberation Items (Questions):

**Question 1:** Do we treat “observability + correctness” as a release-blocking gate for Cloud/Framework, even if it delays new features requested by the community?

  **Context:**
  - `GitHub PR #6316 (0xbbjoker): "Fixes critical bug that broke all database operations with data isolation enabled."`
  - `Discord core-devs (Stan ⚡): "added a linter for logs in the eliza/config package that warns developers when logs are in the wrong format"`

  **Multiple Choice Answers:**
    a) Yes—make correctness/observability a hard gate (no release if isolation/logging standards regress).
        *Implication:* Improves long-term trust and reduces incident load, but may slow visible shipping cadence.
    b) Partially—gate only the Cloud path; keep framework releases flowing with warnings and best-effort fixes.
        *Implication:* Balances shipping and reliability, but risks confusing quality expectations between Cloud and OSS.
    c) No—prioritize feature velocity; handle observability/correctness in parallel unless catastrophic.
        *Implication:* Maximizes short-term momentum, but increases risk of high-profile breakages that damage developer trust.
    d) Other / More discussion needed / None of the above.

**Question 2:** Should the new log linter be enforced across the plugin ecosystem (registry admission and CI), or kept advisory to avoid contributor friction?

  **Context:**
  - `Discord core-devs (Stan ⚡): "linter for logs... can be integrated into projects and plugins to maintain consistent logging standards"`

  **Multiple Choice Answers:**
    a) Enforce in CI for core + official plugins first, then require for registry admission over time.
        *Implication:* Raises baseline quality with a predictable rollout, at the cost of some ecosystem churn.
    b) Keep it advisory; provide an autofix codemod and templates to encourage adoption.
        *Implication:* Preserves contributor velocity, but may not deliver the consistency needed for reliable ops.
    c) Enforce everywhere immediately (core + all plugins), with CI failing on violations.
        *Implication:* Fastest path to standardization, but likely to deter community contributions and flood maintainers with PR noise.
    d) Other / More discussion needed / None of the above.

**Question 3:** How aggressively should we expand parallel execution (Promise.allSettled) patterns beyond MultiStep, given fault-tolerance vs. determinism tradeoffs for agents?

  **Context:**
  - `GitHub PR #6263 (standujar): "Converts sequential provider execution to parallel execution using Promise.allSettled... faster."`

  **Multiple Choice Answers:**
    a) Expand broadly—standardize parallel provider/tool calls wherever safe, with timeouts and isolation.
        *Implication:* Improves responsiveness and perceived intelligence, but increases complexity in reproducibility and debugging.
    b) Keep parallelism limited to provider fetch stages; maintain sequential execution for tool/action stages.
        *Implication:* Preserves deterministic agent behavior while still improving latency in retrieval/augmentation.
    c) Roll back/limit parallelism until we have stronger tracing and determinism controls in place.
        *Implication:* Reduces emergent race-condition risk, but may slow agents and undercut UX competitiveness.
    d) Other / More discussion needed / None of the above.

---


### 2. Topic: Cloud & Public Agent UX: Onboarding Friction, Gating, and Wallet Flow

**Summary of Topic:** A burst of UX issues proposes reshaping public-agent interaction states, limiting messages for unauthenticated users, improving wallet connect flow, and adding basic engagement metrics—these changes directly impact conversion, trust, and the Cloud growth loop.

#### Deliberation Items (Questions):

**Question 1:** Do we adopt a three-state public-agent UX (visitor vs. authenticated non-owner vs. owner) as a near-term Cloud priority to reduce confusion and increase conversion?

  **Context:**
  - `GitHub issue #6313 (borisudovicic): "There are three different states... should have a distinct UI."`
  - `GitHub issue #6312 (borisudovicic): "Limit messages for non-signed up user to ~2-3"`

  **Multiple Choice Answers:**
    a) Yes—implement the three-state model immediately; treat it as a core Cloud conversion fix.
        *Implication:* Likely increases activation and reduces user confusion, but diverts engineering time from other stability work.
    b) Partially—ship only the visitor state cleanup + soft gating; defer owner/non-owner nuance.
        *Implication:* Delivers quick conversion wins with minimal scope, but may leave edge-case UX confusion unresolved.
    c) No—maintain current unified UI until the broader dashboard overhaul lands.
        *Implication:* Avoids churn and rework, but risks ongoing poor first impressions and missed growth during Cloud launch window.
    d) Other / More discussion needed / None of the above.

**Question 2:** What is the Council’s stance on anonymous message limits for public agents: growth lever, abuse control, or mission risk to “developer-friendly” openness?

  **Context:**
  - `GitHub issue #6312 (borisudovicic): "Limit messages for non-signed up user to ~2-3"`

  **Multiple Choice Answers:**
    a) Adopt strict limits (2–3) with a signup wall to maximize conversion and cost control.
        *Implication:* Improves unit economics and growth funnel discipline, but may reduce viral sharing and casual exploration.
    b) Adopt a generous trial (10–20) with rate-limits and abuse detection; optimize for delight first.
        *Implication:* Strengthens “developer-friendly” perception and shareability, but increases compute spend and spam risk.
    c) No hard limits—use only rate-limits + anomaly detection; keep public agents truly public.
        *Implication:* Maximizes openness aligned with OSS ethos, but risks runaway costs and degraded service for legitimate users.
    d) Other / More discussion needed / None of the above.

**Question 3:** Should we prioritize wallet-connect UX improvements now (direct-to-wallet options), or sequence them after agent UX cleanup to avoid splitting focus?

  **Context:**
  - `GitHub issue #6317 (borisudovicic): "Connect wallet should ideally go straight to wallet options"`

  **Multiple Choice Answers:**
    a) Prioritize wallet-connect flow immediately; onboarding cannot be reliable if auth is clumsy.
        *Implication:* Reduces drop-off at a critical step and supports token-related operations, but may interrupt current UX refactors.
    b) Sequence after public-agent UX changes; fix first-contact confusion before optimizing wallet steps.
        *Implication:* Creates a coherent “first session” experience, but may leave conversion leakage in token/migration-adjacent flows.
    c) Bundle both into a single onboarding sprint with clear owner and success metrics.
        *Implication:* Maximizes impact per release and improves narrative, but increases scope risk and schedule uncertainty.
    d) Other / More discussion needed / None of the above.

---


### 3. Topic: Developer Trust Loop: Multi-Model Patterns, Documentation, and Narrative Defense

**Summary of Topic:** Discord signals strong builder interest in multi-model agent configuration, Cloud container deployment for custom plugins, and clearer website/API-key integration—while skepticism about “wrapping models” persists; this is a documentation and positioning moment to reinforce the “Linux of agents” claim with concrete reference implementations.

#### Deliberation Items (Questions):

**Question 1:** Do we formalize multi-model orchestration as a first-class “blessed pattern” (docs + templates), or keep it as an advanced/experimental approach to protect reliability?

  **Context:**
  - `Discord coders (Omid Sa): wants "Anthropic for calculations/forecasting and OpenAI for reasoning"`
  - `Discord coders (Stan ⚡): "Use Openrouter plugin and define provider/LLM model in your env file"`

  **Multiple Choice Answers:**
    a) Bless it now—ship official examples, guardrails, and recommended model routing conventions.
        *Implication:* Accelerates ecosystem innovation and strengthens composability narrative, but expands support surface area.
    b) Provide examples but label as experimental; require explicit opt-in flags and warnings in Cloud.
        *Implication:* Enables builders while protecting reliability expectations and reducing surprise costs/perf variance.
    c) Defer—focus on single-provider stability until Cloud launch is fully hardened.
        *Implication:* Reduces near-term support burden, but risks losing advanced builders to competing frameworks.
    d) Other / More discussion needed / None of the above.

**Question 2:** How should we respond structurally (not rhetorically) to skepticism that ElizaOS is merely a model wrapper—what proof artifacts do we prioritize shipping?

  **Context:**
  - `Daily report (Jan 3): "Some community members have expressed skepticism... questioning whether it provides substantial value beyond wrapping existing AI models."`
  - `Discord coders: Stan describes ElizaOS as providing "databases, embeddings, model abstraction, APIs, composable tool concepts"`

  **Multiple Choice Answers:**
    a) Ship benchmarked reference agents + reproducible demos showing persistence, tools, and multi-platform deployment.
        *Implication:* Turns narrative into evidence and improves adoption, but requires product polish and ongoing maintenance.
    b) Ship developer docs first (architecture diagrams, primitives, DX guides) and let community produce demos.
        *Implication:* Scales explanation and reduces core team demo burden, but may not persuade skeptics without “wow” artifacts.
    c) Prioritize Cloud reliability and onboarding metrics; skepticism will fade as usage grows.
        *Implication:* Optimizes for execution excellence, but risks leaving a vacuum where competitors define the narrative.
    d) Other / More discussion needed / None of the above.

**Question 3:** Should we commission an ElizaOS-owned “agentic AI crash course” as an official lead funnel, and if so, what is the governance/maintenance model to keep it trustworthy?

  **Context:**
  - `Discord core-devs (Kenk): shared an "agentic AI crash course" and suggested integration with Eliza for lead generation`
  - `Daily report (Jan 3): "course would serve as lead funnel for their cloud services"`

  **Multiple Choice Answers:**
    a) Yes—official course with quarterly updates; owned by core team with clear versioning tied to Framework/Cloud releases.
        *Implication:* Strong growth lever and consistent DX narrative, but creates a recurring maintenance commitment.
    b) Yes—but community-led with Council-approved syllabus and automated doc tests/examples to prevent drift.
        *Implication:* Scales content production while preserving trust, but requires governance processes and tooling.
    c) No—focus on docs and reference repos; avoid marketing-adjacent commitments until Cloud is stable.
        *Implication:* Protects execution focus, but may slow ecosystem growth and fail to capitalize on current attention.
    d) Other / More discussion needed / None of the above.