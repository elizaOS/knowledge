name: Extract Daily Facts

on:
  schedule:
    # Runs daily at 8:45 UTC (after aggregation at 8:30)
    - cron: '45 8 * * *'
  workflow_dispatch: # Allows manual triggering

concurrency:
  group: knowledge-repo-writes
  cancel-in-progress: false

jobs:
  extract-facts:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Need permission to write back to the repo

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Pull latest changes
        run: git pull --rebase origin main || true

      - name: Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.12'
          packages: 'requests'

      - name: Get aggregated data file
        id: aggregate_step
        run: |
          TODAY=$(date +%Y-%m-%d)
          DAILY_FILE="the-council/aggregated/${TODAY}.json"
          PERMALINK="the-council/aggregated/daily.json"

          if [ -f "$DAILY_FILE" ]; then
            echo "aggregated_json_file=$DAILY_FILE" >> $GITHUB_OUTPUT
          elif [ -f "$PERMALINK" ]; then
            echo "Using daily.json permalink as fallback"
            echo "aggregated_json_file=$PERMALINK" >> $GITHUB_OUTPUT
          else
            echo "No aggregated file found, running aggregation..."
            python scripts/etl/aggregate-sources.py
            echo "aggregated_json_file=$DAILY_FILE" >> $GITHUB_OUTPUT
          fi

      - name: Generate Fact Briefing from Aggregated Data
        id: fact_gen_step
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          INPUT_FILE="${{ steps.aggregate_step.outputs.aggregated_json_file }}"
          OUTPUT_JSON_DIR="the-council/facts"
          OUTPUT_MARKDOWN_DIR="hackmd/facts"

          mkdir -p "$OUTPUT_JSON_DIR"
          mkdir -p "$OUTPUT_MARKDOWN_DIR"

          if [ -f "$INPUT_FILE" ]; then
            OUTPUT_JSON_FILE="$OUTPUT_JSON_DIR/$(date +%Y-%m-%d).json"
            OUTPUT_MARKDOWN_FILE="$OUTPUT_MARKDOWN_DIR/$(date +%Y-%m-%d).md"

            python scripts/etl/extract-facts.py -i "$INPUT_FILE" -o "$OUTPUT_JSON_FILE" -md "$OUTPUT_MARKDOWN_FILE"

            echo "fact_json_file=$OUTPUT_JSON_FILE" >> $GITHUB_OUTPUT
            echo "fact_markdown_file=$OUTPUT_MARKDOWN_FILE" >> $GITHUB_OUTPUT
          else
            echo "Input file $INPUT_FILE not found, skipping fact briefing generation."
            exit 1
          fi
      - name: Create permalink for JSON fact file
        run: |
          DAILY_FACT_JSON="the-council/facts/$(date +%Y-%m-%d).json"
          PERMALINK_FACT_JSON="the-council/facts/daily.json"
          if [ -f "$DAILY_FACT_JSON" ]; then
            cp "$DAILY_FACT_JSON" "$PERMALINK_FACT_JSON"
          fi

      - name: Commit updated files
        uses: stefanzweifel/git-auto-commit-action@v5
      - name: Alert on failure
        if: failure()
        uses: ./.github/actions/alert-failure
        with:
          webhook-url: ${{ secrets.ALERT_WEBHOOK_URL }}
          workflow-name: 'Extract Daily Facts'