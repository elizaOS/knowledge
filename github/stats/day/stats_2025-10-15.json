{
  "interval": {
    "intervalStart": "2025-10-15T00:00:00.000Z",
    "intervalEnd": "2025-10-16T00:00:00.000Z",
    "intervalType": "day"
  },
  "repository": "elizaos/eliza",
  "overview": "From 2025-10-15 to 2025-10-16, elizaos/eliza had 1 new PRs (1 merged), 0 new issues, and 3 active contributors.",
  "topIssues": [
    {
      "id": "I_kwDOMT5cIs7F0i3j",
      "title": "Eigencloud POC",
      "author": "borisudovicic",
      "number": 5768,
      "repository": "elizaos/eliza",
      "body": "This service offers a *deterministic* OpenAI-compliant LLM inference endpoint that can be used via multiple clients. A couple of key points:\n\n* By OpenAI compliancy we specifically mean the messages-based Chat Completions API: [https://platform.openai.com/docs/api-reference/chat/create](https://platform.openai.com/docs/api-reference/chat/create),\n* By ‚Äúdeterministic‚Äù we specifically mean that one request (prompt, parameters, etc) provided to the API multiple times will produce the **same** output.\n  * For eg. setting a different seed but otherwise keeping the request the same will produce a different output.\n\n## curl\n\n```\ncurl -s -X POST http://192.222.58.254:8001/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"gemma-3-27b-it\",\n  \"max_completion_tokens\": 120,\n  \"seed\": 42,\n  \"messages\": [{\"role\": \"user\", \"content\": \"Write a story about programming\"}]\n}' | jq\n```\n\n### Streaming response\n\n```\ncurl -s -X POST http://192.222.58.254:8001/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"gemma-3-27b-it\",\n  \"max_completion_tokens\": 120,\n  \"seed\": 42,\n  \"stream\": true,\n  \"messages\": [{\"role\": \"user\", \"content\": \"Write a story about programming\"}]\n}'\n```\n\n## Vercel AI SDK\n\n[https://ai-sdk.dev/docs/introduction](https://ai-sdk.dev/docs/introduction)\n\n```\nimport { generateText } from 'ai';\nimport { createOpenAI } from '@ai-sdk/openai';\n\nconst config = {\n  baseURL: 'http://192.222.58.254:8001/v1',\n  apiKey: 'sk-dummy-custom-key',\n  model: 'gemma-3-27b-it'\n};\n\n// Create a custom OpenAI client instance\nconst customOpenAI = createOpenAI({\n  baseURL: config.baseURL,\n  apiKey: config.apiKey,\n});\n\n// Create the model instance using the custom client\nconst model = customOpenAI(config.model);\n\nasync function chatCompletion() {\n  console.log('\\nüí¨ Testing chat completion...');\n  \n  try {\n    const result = await generateText({\n      model: model,\n      messages: [\n        { role: 'system', content: 'You are a helpful assistant.' },\n        { role: 'user', content: 'What are the benefits of using local AI models?' }\n      ],\n      seed: 42\n    });\n\n    console.log('‚úÖ Chat response:');\n    console.log(result.text);\n  } catch (error) {\n    console.error('‚ùå Error with chat completion:');\n    console.error(error.message);\n  }\n}\n\n// Main execution\nasync function main() {\n  console.log('EigenAI usage via Vercel AI SDK\\n');\n  console.log('=' + '='.repeat(49));\n  \n  console.log(`\\nüîß Configuration:`);\n  console.log(`   Service: EigenAI`);\n  console.log(`   Base URL: ${config.baseURL}`);\n  console.log(`   Model: ${config.model}`);\n  console.log(`   API Key: ${config.apiKey.substring(0, 10)}...`);\n  \n  await chatCompletion();\n}\n\nmain().catch(console.error);\n```",
      "createdAt": "2025-08-13T15:21:30Z",
      "closedAt": "2025-10-15T14:50:04Z",
      "state": "CLOSED",
      "commentCount": 0
    }
  ],
  "topPRs": [
    {
      "id": "PR_kwDOMT5cIs6t47Ks",
      "title": "feat(cli): add Eigen TEE wrapper",
      "author": "wtfsayo",
      "number": 6065,
      "body": "## Summary\n- add Eigen CLI wrapper to tee command for TEE deployments\n- provide consented installation flow and PATH detection for Eigen binaries\n\n## Testing\n- manual: \n  - bun run eliza/packages/cli/dist/index.js tee eigen (prompts, installs, re-run)\n\n<!-- CURSOR_SUMMARY -->\n---\n\n> [!NOTE]\n> Adds `tee eigen` wrapper that proxies to `eigenx`, with auto-detect/consented install and PATH handling.\n> \n> - **CLI (TEE)**:\n>   - **New `eigen` wrapper** in `packages/cli/src/commands/tee/eigen-wrapper.ts`:\n>     - Proxies args to `eigenx`/`eigenx-dev`, resolves binary via env/known paths, and augments `PATH`.\n>     - Interactive or env-driven consented install via curl if binary missing; robust error handling and logging.\n>     - Pass-through execution with inherited stdio; custom help output and examples.\n>   - **Integration**: Adds `.addCommand(eigenCliCommand)` in `packages/cli/src/commands/tee/index.ts`.}\n> \n> <sup>Written by [Cursor Bugbot](https://cursor.com/dashboard?tab=bugbot) for commit 75980ffcb55a17a77f8c12672448139a0db7ab0b. This will update automatically on new commits. Configure [here](https://cursor.com/dashboard?tab=bugbot).</sup>\n<!-- /CURSOR_SUMMARY -->",
      "repository": "elizaos/eliza",
      "createdAt": "2025-10-15T13:41:16Z",
      "mergedAt": "2025-10-15T17:34:53Z",
      "additions": 246,
      "deletions": 1
    }
  ],
  "codeChanges": {
    "additions": 246,
    "deletions": 1,
    "files": 2,
    "commitCount": 6
  },
  "completedItems": [
    {
      "title": "feat(cli): add Eigen TEE wrapper",
      "prNumber": 6065,
      "type": "feature",
      "body": "## Summary\n- add Eigen CLI wrapper to tee command for TEE deployments\n- provide consented installation flow and PATH detection for Eigen binaries\n\n## Testing\n- manual: \n  - bun run eliza/packages/cli/dist/index.js tee eigen (prompts, instal",
      "files": [
        "packages/cli/src/commands/tee/eigen-wrapper.ts",
        "packages/cli/src/commands/tee/index.ts"
      ]
    }
  ],
  "topContributors": [
    {
      "username": "0xbbjoker",
      "avatarUrl": "https://avatars.githubusercontent.com/u/54844437?u=90fe1762420de6ad493a1c1582f1f70c0d87d8e2&v=4",
      "totalScore": 40.18701911726089,
      "prScore": 34.987019117260886,
      "issueScore": 0,
      "reviewScore": 5,
      "commentScore": 0.2,
      "summary": null
    },
    {
      "username": "wtfsayo",
      "avatarUrl": "https://avatars.githubusercontent.com/u/82053242?u=98209a1f10456f42d4d2fa71db4d5bf4a672cbc3&v=4",
      "totalScore": 34.05942874616498,
      "prScore": 34.05942874616498,
      "issueScore": 0,
      "reviewScore": 0,
      "commentScore": 0,
      "summary": null
    }
  ],
  "newPRs": 1,
  "mergedPRs": 1,
  "newIssues": 0,
  "closedIssues": 1,
  "activeContributors": 3
}